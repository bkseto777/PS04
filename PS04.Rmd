---
title: "STAT/MATH 495: Problem Set 04"
author: "Brendan Seto"
date: "2017-10-03"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Collaboration

Please indicate who you collaborated with on this assignment:
Leonard Yoon


```{r, warning=FALSE}
library(tidyverse)
library(rlang)
library(cowplot)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```

# Introduction
I trained a linear model with 7 different formulas.  Each formulas differed by number of predictors.  The purpose of the exercise was to determine the optimal amount of coefficients to include in order to get the best results, measured by minimum RMSE.  

```{r}
model1_formula <- as.formula("Balance ~ 1")
model2_formula <- as.formula("Balance ~ Income")
model3_formula <- as.formula("Balance ~ Income + Limit")
model4_formula <- as.formula("Balance ~ Income + Limit + Rating")
model5_formula <- as.formula("Balance ~ Income + Limit + Rating + Age")
model6_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model7_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")

form <- c(model1_formula, model2_formula, model3_formula, model4_formula,model5_formula, model6_formula, model7_formula)
```


```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7. For now, I've filled them with arbitrary 
# values; you will fill these in
RMSE <- function(x, test){
  model_loess <- lm(form[[x]], data=credit_train)
  
  p <- test %>% mutate(prediction = predict(model_loess, newdata=test), dif = (Balance - prediction)^2)%>% 
  summarise(RMSE = sqrt(mean(dif)))
  
  return(p$RMSE)
}

num_coef = 1:7

# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
  num_coefficients = num_coef,
  `Test data`  = sapply(num_coef, RMSE, test=credit_test),
  `Train data` = sapply(num_coef, RMSE, test=credit_train)
) 

# Some cleaning of results
results <- results %>% gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model", title = "RMSE vs # of coefficients for LOESS Models (train dataset n=20)")
```


# Interpret the graph

Both curves drop precipitously after the 2nd coefficient.  This suggests the third addition gives a lot of additional information and is important to include.  However, the fourth coefficient does not help all that much.  Thus it may not be necessary.  In fact, when we test the model on the test data, more coefficients lead to a higher RMSE.  This may be due to overfitting, where we fit the model to noise rather than signal.  Thus we start to lose predictive power as noise is not shared between samples.  


# Bonus

Repeat the whole process, but let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.

```{r}
set.seed(79)
credit_trainB <- credit %>% 
  sample_n(380)
credit_testB <- credit %>% 
  anti_join(credit_train, by="ID")
```

```{r}
# Save results in a data frame. Note this data frame is in wide format.
resultsB <- data_frame(
  num_coefficients = num_coef,
  `Test data`  = sapply(num_coef, RMSE, test=credit_testB),
  `Train data` = sapply(num_coef, RMSE, test=credit_trainB)
) 

# Some cleaning of results
resultsB <- resultsB %>% gather(type, RMSE, -num_coefficients)

ggplot(resultsB, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  geom_point(data=results, aes(x=num_coefficients, y=RMSE, col=type))+
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model", title = "RMSE vs # of coefficients for LOESS Models (train dataset n=380)")

```

This time, the curves were very similar to each other.  Even if we add additional coefficients, the curves do not differ by that much.  This may be due to the fact that there is far less noise in a training sample of size 380 as opposed to 20.  Thus overfitting is not as large of a problem.  However, this means that the test dataset has much more noise, so even though we have a better model our predictions are not necessarily more accurate.  

In the graph, the original train-test estimates are displayed as points for visibility. Interestingly, switching the training and testing datasets does not make a difference.  This suggests that training on a large dataset before testing on a small one is equivalent to training on a small dataset and testing on a large one.  

It is interesting that training and testing the model on the same dataset does differ significantly by size.  This may be an indicator that we are fitting less to the noise and more to the signal, as we had hoped.  
